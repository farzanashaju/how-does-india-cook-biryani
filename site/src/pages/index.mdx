---
layout: ../layouts/Layout.astro
title: How Does India Cook Biryani?
---

# How Does India Cook Biryani?

**Authors:** C.V. Rishi†, Farzana S†, Shubham Goel†, Aditya Arun, C.V. Jawahar  
**Affiliation:** IIIT Hyderabad  
**Conference:** ICVGIP'25 (16th Indian Conference on Computer Vision, Graphics and Image Processing)  
† Joint First Authors

<div class="btn-group">
  <a
    href="https://github.com/farzanashaju/how-does-india-cook-biryani"
    class="btn"
  >
    GitHub
  </a>
  <a
    href="https://github.com/farzanashaju/how-does-india-cook-biryani"
    class="btn"
  >
    arXiv
  </a>
  <a
    href="https://github.com/farzanashaju/how-does-india-cook-biryani"
    class="btn"
  >
    PDF
  </a>
</div>

## Abstract

Biryani, one of India's most celebrated dishes, exhibits remarkable regional diversity in its preparation, ingredients, and presentation. With the growing availability of online cooking videos, there is unprecedented potential to study such culinary variations using computational tools systematically. However, existing video understanding methods fail to capture the fine-grained, multimodal, and culturally grounded differences in procedural cooking videos.

This work presents the first large-scale, curated dataset of biryani preparation videos, comprising **120 high-quality YouTube recordings across 12 distinct regional styles** (Ambur, Bombay, Dindigul, Donne, Hyderabadi, Kashmiri, Kolkata, Awadhi, Malabar, Mughlai, Sindhi, and Thalassery). We propose a multi-stage framework leveraging recent advances in vision–language models (VLMs) to segment videos into fine-grained procedural units and align them with audio transcripts and canonical recipe text.

Building on these aligned representations, we introduce a video comparison pipeline that automatically identifies and explains procedural differences between regional variants. We construct a comprehensive question–answer (QA) benchmark spanning multiple reasoning levels to evaluate procedural understanding in VLMs. Our approach employs multiple VLMs in complementary roles, incorporates human-in-the-loop verification for high-precision tasks, and benchmarks several state-of-the-art models under zero-shot and fine-tuned settings.

**Key Contributions:**

1. **Dataset:** We introduce the first curated dataset of Indian biryani preparation videos, annotated with fine-grained temporal segmentation and multimodal labels
2. **Pipeline:** We design a robust VLM-based pipeline for procedural video segmentation, multimodal alignment, and question-answer generation
3. **Comparison Framework:** We propose a novel video comparison framework for analysing subtle procedural differences across regional biryani variants
4. **Benchmarks:** We provide quantitative benchmarks and qualitative analyses of the performance of current VLMs on culturally rich procedural video understanding tasks
5. **Foundation:** The resulting dataset, comparison methodology, and QA benchmark provide a new testbed for evaluating VLMs on structured, multimodal reasoning tasks and open new directions for computational analysis of cultural heritage through cooking videos

---

## Problem Overview

Biryani is more than a culinary dish; it is a cultural symbol that embodies the diversity and richness of Indian gastronomy. While its name is shared across the country, its preparation varies widely across regions, shaped by local traditions, availability of ingredients, and individual cooking styles. These differences manifest in **flavour and the sequence of preparation steps, the choice of utensils, and the presentation style**. With the proliferation of online platforms such as YouTube, this diversity is now documented at scale through cooking videos, providing an invaluable record of culinary practices. However, despite abundant content, the computational tools required to systematically capture and compare fine-grained procedural variations in such videos remain underdeveloped.

### Challenges in Video Understanding

Cooking videos present a unique challenge for computer vision due to their **multimodal nature, temporal complexity, and diversity in visual presentation**. The same high-level dish can be prepared using markedly different sequences of actions, ingredient combinations, and stylistic elements, often accompanied by narration in different languages or dialects.

Indian cooking is known for its **multi-step processes and intricate use of spices**, and these details are central to understanding the cultural and procedural identity of a recipe. Conventional video understanding approaches have primarily focused on coarse-grained action recognition or highlight detection, which are **insufficient for modelling such nuanced, structured tasks**.

### Motivation for VLMs

Over the past two decades, video analysis has evolved from handcrafted feature-based methods (Hidden Markov Models, Support Vector Machines) to deep learning models capable of capturing richer visual patterns from large datasets. More recently, **large vision–language models (VLMs) have emerged as a powerful paradigm**, jointly reasoning over visual and textual information to produce semantically meaningful outputs. These models have demonstrated strong generalisation capabilities in diverse domains, yet their application to **structured procedural understanding remains relatively unexplored, particularly in culturally rich contexts**.

In the context of cooking and biryani in particular, VLMs can move beyond recognising individual actions toward modelling the **full procedural flow, aligning it with textual recipes, and enabling fine-grained comparisons between variations**.

### Our Approach

This work addresses these challenges by:

1. **Creating a comprehensive dataset** - 120 biryani cooking videos across 12 regional types (Ambur, Bombay, Dindigul, Donne, Hyderabadi, Kashmiri, Kolkata, Awadhi, Malabar, Mughlai, Sindhi, and Thalassery)
2. **Developing multimodal alignment** - Aligning video, audio transcripts, and recipe text to enable fine-grained understanding
3. **Enabling comparative analysis** - Building tools to automatically identify and explain procedural differences between regional variants
4. **Benchmarking VLMs** - Evaluating state-of-the-art models on structured multimodal reasoning tasks

<div class="img-grid">
  <div>
    <img src="/images/biryani-map.png" alt="Regional Biryani Types Map" />
    <p class="img-caption">
      Figure 1: Map of India showing 12 regional biryani types - Ambur, Bombay,
      Dindigul, Donne, Hyderabadi, Kashmiri, Kolkata, Awadhi, Malabar, Mughlai,
      Sindhi, and Thalassery
    </p>
  </div>
  <div>
    <img src="/images/dataset.png" alt="Dataset Overview" />
    <p class="img-caption">
      Figure 2: Overview of the Biryani Dataset showing regional diversity in
      presentation, colour palette, and plating
    </p>
  </div>
</div>

---

## Dataset Overview

Our dataset spans **12 distinct types of biryani** across India, with **10 videos per category** (120 videos total). Each video features:

- **High-quality recordings** with clear audio narration in regional languages
- **Complete visual coverage** of the preparation process
- **Duration range** of 5-12 minutes (most videos)
- **Regional diversity** in presentation, colour palette, and plating traditions
- **Sourced from YouTube** to capture authentic regional cooking practices

### Dataset Curation

Videos were chosen for their **culinary popularity and the availability of high-quality recordings**. To maximise utility for downstream tasks, we prioritized videos featuring:

- Clear audio narration of cooking steps
- Spoken narration of cooking steps
- Complete visual coverage of the preparation process
- A range of durations

Given the pan-Indian diversity of the selected biryani types, the dataset exhibits **substantial variation in language, cooking techniques, narration styles, and cinematographic choices** such as camera angles and editing styles.

### Multimodal Processing

We extract audio from each video and perform **automatic speech recognition (ASR) using WhisperX**, falling back to Whisper-Large when needed. All transcripts are **translated into English (using GPT-4)** to standardise linguistic representation across the dataset. We then use **part-of-speech tagging with spaCy** to extract nouns, verbs, and adjectives from the transcripts, producing frequency-based visualisations.

### Canonical Template Recipes

We generate **canonical template recipes for each biryani type using GPT-4**, which provided structured reference sequences of cooking steps. These generated templates served as a standardized framework for identifying procedural steps across diverse video formats. Manual verification ensured the consistency and usability of this framework for temporal segmentation.

The dataset includes regional variants: **Ambur, Bombay, Dindigul, Donne, Hyderabadi, Kashmiri, Kolkata, Awadhi, Malabar, Mughlai, Sindhi, and Thalassery**.

---

## Citation

If you use this work in your research, please cite:

```bibtex
@inproceedings{rishi2025biryani,
  title={How Does India Cook Biryani?},
  author={Rishi, C.V. and Farzana, S. and Goel, Shubham and Arun, Aditya and Jawahar, C.V.},
  booktitle={Proceedings of 16th Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP'25)},
  year={2025},
  organization={ACM}
}
```

---

## Acknowledgements

We thank the contributors and the cooking video creators on YouTube who made this dataset possible. This work was supported by IIIT Hyderabad.
