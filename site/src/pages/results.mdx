---
layout: ../layouts/Layout.astro
title: Results - How Does India Cook Biryani?
---

# Results

## Stage 1: Video Segmentation Results

### Action Clustering

The initial action detection stage produced a highly granular label space, with **10,481 unique action classes**. After applying the action clustering process, this number was reduced to **2,187 canonicalised action classes**, greatly improving consistency in labelling. This represents a **79.1% reduction** in label space complexity.

**Analysis:** The action clustering using SentenceTransformer embeddings and agglomerative clustering successfully consolidated semantically similar but lexically different action descriptions. For example:

- "stirring rice" + "stirring rice and water with a wooden spoon" → canonical "stirring rice"
- "adding onions" + "adding fried onions" → canonical "adding onions"

### Temporal Merging

Similarly, the temporal merging process significantly reduced fragmentation in the video segmentation. Across all videos, the number of timestamped clips decreased from **16,761 before merging** to **14,479 after merging**, representing a **13.6% reduction** in segment count while preserving full action coverage.

### Action Verification

We verified **14,470 video–action segments** across all biryani types, with **11,295 (78.05%)** labelled as correct and **3,175 (21.95%)** as incorrect, thereby increasing confidence in the dataset's action labels. The 78% accuracy rate demonstrates that InternVL-14B produces reliable visual annotations for cooking-specific tasks.

---

## Video Comparison Results

### Detection Statistics

Our video comparison framework identified meaningful differences across biryani varieties. The framework detected differences in **33.2% of action comparisons**. This proportion indicates that while biryani varieties share core cooking procedures, they exhibit distinct variations in execution methods.

| Outcome                  | Percentage of Comparison |
| ------------------------ | ------------------------ |
| Difference detected      | 33.2%                    |
| No detectable difference | 66.8%                    |

The detection rate aligns with expectations for regional culinary variants, where fundamental processes remain consistent but specific techniques diverge based on cultural and regional influences.

### Validation Accuracy

To validate the accuracy of the framework, **2,000 randomly sampled comparisons** were verified by a group of 4–5 independent annotators. The verification results reveal systematic challenges in the model's performance:

| Category            | Correct | Incorrect |
| ------------------- | ------- | --------- |
| Difference detected | 67.5%   | 32.5%     |
| No difference       | 45.7%   | 54.3%     |

The framework achieved **67.5% accuracy** for detected differences, indicating reliable identification of actual procedural variations. However, accuracy drops to **45.7%** for "no difference" classifications, suggesting the model misses subtle but meaningful variations that human annotators can detect.

This performance gap likely stems from the model's limited exposure to Indian cooking contexts during training, resulting in conservative judgments when analysing culturally specific culinary techniques.

---

## Stage 2: Multimodal Alignment Results

The multimodal alignment process successfully unified three heterogeneous modalities (visual segmentation, audio transcripts, and recipe text). Using Dynamic Time Warping with semantic embeddings, we achieved robust alignment despite:

- **Variable narration pacing** across different videos
- **Different granularities** of instruction descriptions
- **Language diversity** (pre-translation to English)
- **Diverse cinematographic styles**

This alignment enables recipe-aware search, allowing users to retrieve semantically relevant moments across the entire video corpus.

---

## Stage 3: Video Comparison Results

### Detection Statistics

Our video comparison framework identified meaningful differences across biryani varieties. The framework detected differences in **33.2% of action comparisons**. This proportion indicates that while biryani varieties share core cooking procedures, they exhibit distinct variations in execution methods.

| Outcome                  | Percentage of Comparison |
| ------------------------ | ------------------------ |
| Difference detected      | 33.2%                    |
| No detectable difference | 66.8%                    |

The detection rate aligns with expectations for regional culinary variants, where **fundamental processes remain consistent but specific techniques diverge** based on cultural and regional influences.

### Validation Accuracy

To validate the accuracy of the framework, **2,000 randomly sampled comparisons** were verified by a group of 4–5 independent annotators. The verification results reveal systematic challenges in the model's performance:

| Category            | Correct | Incorrect |
| ------------------- | ------- | --------- |
| Difference detected | 67.5%   | 32.5%     |
| No difference       | 45.7%   | 54.3%     |

**Key Findings:**

- The framework achieved **67.5% accuracy** for detected differences, indicating reliable identification of actual procedural variations
- Accuracy drops to **45.7%** for "no difference" classifications, suggesting the model misses subtle but meaningful variations that human annotators can detect
- This performance gap likely stems from the model's limited exposure to Indian cooking contexts during training, resulting in conservative judgments when analysing culturally specific culinary techniques
- The model occasionally generates false differences or misattributes variations between video clips

---

## Video Question Answering (QA) Benchmark

### Dataset Statistics

Our QA generation pipeline produces:

- **240 easy** question–answer pairs (11.5%)
- **1,357 medium** question–answer pairs (65.1%)
- **486 hard** question–answer pairs (23.3%)

The hard QA set is further subdivided based on the number of videos required for reasoning:

- **hardqa2:** 146 questions (2 videos)
- **hardqa3:** 171 questions (3 videos)
- **hardqa4:** 82 questions (4 videos)
- **hardqa5:** 87 questions (5 videos)

The dataset is evenly split into training and test sets to support model development and evaluation.

### QA Generation Strategy

Our multi-stage pipeline spans:

1. **Temporal segmentation** - Breaking videos into meaningful procedural chunks
2. **Automated visual description** - Using InternVL3-14B to caption segments
3. **Language model prompting** - Aggregating captions into coherent narratives
4. **Human curation** - Ensuring quality and removing duplicates

**Difficulty Tiers:**

- **Easy:** Single short segment (10-second clips) - basic perceptual recognition
- **Medium:** Entire video comprehension - temporal and procedural understanding
- **Hard:** Multi-video reasoning - cross-recipe comparison and synthesis

### Answer Length Distribution

As expected, harder questions tend to require longer answers:

- **Easy**: ~12 words average
- **Medium**: ~15 words average
- **Hard**: ~20+ words average

This trend reflects the **increased complexity and reasoning demands** of higher difficulty levels. Easy QA focuses on isolated observations, medium QA integrates temporal sequences, and hard QA synthesizes information from multiple videos.

<div class="img-grid">
  <div>
    <img src="/images/qa-examples.png" alt="QA Examples" />
    <p class="img-caption">
      Figure 6: Example QA pairs from the biryani video QA dataset, covering
      easy, medium, and hard difficulty tiers. Questions were generated via a
      multi-stage pipeline (temporal segmentation, captioning, summary
      synthesis, LLM prompting, and human curation).
    </p>
  </div>
  <div>
    <img src="/images/qa-stats.png" alt="QA Statistics" />
    <p class="img-caption">
      Figure 7: Statistics of the biryani video QA dataset. (a) Distribution of
      question–answer pairs across difficulty levels. (b) Average answer length
      per difficulty type, showing a clear upward trend with complexity.
    </p>
  </div>
</div>

---

## Quantitative Results: VLM Performance

### Overall Performance

We benchmarked several state-of-the-art VLMs on our QA dataset under both zero-shot and fine-tuned settings:

| VLM                       | Metric    | Easy       | Medium     | Hard       |
| ------------------------- | --------- | ---------- | ---------- | ---------- |
| **llama3ft** (Fine-tuned) | BLEU      | **0.0472** | **0.1683** | **0.1140** |
|                           | ROUGE-L   | **0.2689** | **0.4214** | **0.4072** |
|                           | BERTScore | **0.2660** | **0.4869** | **0.4526** |
| qwen2vl                   | BLEU      | 0.0314     | 0.0209     | 0.0609     |
|                           | ROUGE-L   | 0.1914     | 0.1189     | 0.3201     |
|                           | BERTScore | 0.1298     | -0.0747    | 0.3022     |
| internvl3                 | BLEU      | 0.0294     | 0.0291     | 0.0395     |
|                           | ROUGE-L   | 0.2184     | 0.1732     | 0.2457     |
|                           | BERTScore | 0.1663     | 0.1628     | 0.2683     |
| videollama                | BLEU      | 0.0194     | 0.0787     | 0.0502     |
|                           | ROUGE-L   | 0.1883     | 0.2713     | 0.2650     |
|                           | BERTScore | 0.0897     | 0.3071     | 0.2445     |
| llavanext                 | BLEU      | 0.0128     | 0.0216     | 0.0150     |
|                           | ROUGE-L   | 0.1319     | 0.1367     | 0.1911     |
|                           | BERTScore | -0.1732    | 0.0465     | 0.0984     |
| llavaov                   | BLEU      | 0.0038     | 0.0278     | 0.0246     |
|                           | ROUGE-L   | 0.0408     | 0.1383     | 0.1386     |
|                           | BERTScore | -0.2586    | 0.0377     | -0.0073    |

### Models Evaluated

- **llama3ft:** Llama-3.2-11B-Vision-Instruct (fine-tuned on our dataset)
- **qwen2vl:** Qwen2-VL-7B-Instruct (zero-shot)
- **internvl3:** InternVL3-8B (zero-shot)
- **videollama:** VideoLLaMA3-7B-Image (zero-shot)
- **llavanext:** llava-v1.6-mistral-7b-hf (zero-shot)
- **llavaov:** llava-onevision-qwen2-7b-ov-hf (zero-shot)

### Evaluation Metrics

- **BLEU:** Captures lexical precision and word overlap
- **ROUGE-L:** Measures longest common subsequence and recall
- **BERTScore:** Evaluates semantic similarity using contextual embeddings

### Key Findings

1. **Fine-tuned model (llama3ft) outperforms all zero-shot baselines** across all metrics and difficulty levels
2. **Improvements are most pronounced in BERTScore**, indicating stronger semantic alignment in addition to lexical accuracy
3. **Medium and hard tiers** deliberately require temporal, procedural, and cross-recipe reasoning, making the tier structure a stronger indicator of reasoning depth than raw metric scores
4. **Performance generally declines with more videos** in hard questions (hard2 → hard5), reflecting the difficulty of multi-video reasoning
5. Some zero-shot models (e.g., Qwen2-VL, InternVL3) perform **competitively in certain tiers**, but none match the fine-tuned model's consistency

### Hard-Tier Breakdown

Performance across reasoning complexity:

| VLM          | Metric    | hard2  | hard3  | hard4   | hard5   |
| ------------ | --------- | ------ | ------ | ------- | ------- |
| **llama3ft** | BLEU      | 0.1073 | 0.1306 | 0.0987  | 0.1068  |
|              | ROUGE-L   | 0.4045 | 0.4279 | 0.3845  | 0.3927  |
|              | BERTScore | 0.4622 | 0.4669 | 0.4279  | 0.4319  |
| internvl3    | BLEU      | 0.0432 | 0.0405 | 0.0386  | 0.0322  |
|              | ROUGE-L   | 0.2624 | 0.2510 | 0.2444  | 0.2087  |
|              | BERTScore | 0.2882 | 0.2756 | 0.2532  | 0.2347  |
| qwen2vl      | BLEU      | 0.0597 | 0.0679 | 0.0526  | 0.0570  |
|              | ROUGE-L   | 0.3300 | 0.3238 | 0.3174  | 0.2990  |
|              | BERTScore | 0.3107 | 0.2980 | 0.3052  | 0.2932  |
| llavanext    | BLEU      | 0.0052 | 0.0205 | 0.0113  | 0.0239  |
|              | ROUGE-L   | 0.1663 | 0.2038 | 0.1718  | 0.2257  |
|              | BERTScore | 0.0700 | 0.1066 | 0.0727  | 0.1540  |
| llavaov      | BLEU      | 0.0226 | 0.0282 | 0.0215  | 0.0236  |
|              | ROUGE-L   | 0.1390 | 0.1459 | 0.1329  | 0.1286  |
|              | BERTScore | 0.0066 | 0.0094 | -0.0400 | -0.0327 |
| videollama   | BLEU      | 0.0504 | 0.0624 | 0.0339  | 0.0411  |
|              | ROUGE-L   | 0.2643 | 0.2870 | 0.2326  | 0.2537  |
|              | BERTScore | 0.2573 | 0.2552 | 0.2049  | 0.2391  |

---

## Qualitative Results

### Video Comparison Visualization

Our framework successfully captures meaningful procedural differences across regional biryani varieties, providing valuable insights into how traditional cooking methods vary while maintaining cultural authenticity.

<div class="img-grid">
  <div>
    <img
      src="/images/Biryani_Variation_fig.png"
      alt="Biryani Variation Visualization"
    />
    <p class="img-caption">
      Figure 5: Visualization of cooking process variations between Hyderabadi
      and Lucknowi biryani across several cooking stages. Each coloured section
      represents a major cooking stage, with individual squares showing specific
      actions. The opacity of the square is proportional to the degree of
      variation detected between the two biryani styles.
    </p>
  </div>
</div>

### Skill-Based Video Retrieval

Beyond full-recipe visualisation, our dataset supports targeted instructional search within and across videos. For instance, if a user is interested in understanding how to marinate chicken—a critical step in many biryani variants—they can retrieve all video segments across the dataset that involve marination actions. These segments are sourced from different videos but are uniformly timestamped and labelled using our alignment framework.

<div class="img-grid">
  <div>
    <img src="/images/marinade.png" alt="Skill-Based Video Retrieval Example" />
    <p class="img-caption">
      Figure 8: Example of skill-based video retrieval for the query "marinating
      chicken". The system returns short, timestamped clips from multiple
      biryani videos where the marination step is visually identified, enabling
      direct access to semantically relevant moments rather than full unindexed
      videos.
    </p>
  </div>
</div>

Unlike traditional video search engines, which return entire videos without pinpointing where the relevant action occurs, our approach enables direct navigation to semantically aligned moments within the video corpus.

---

## Limitations

Our framework, while comprehensive, has several limitations:

1. **VLM Biases**: The visual language models used in Stage 3 may exhibit biases based on their training data, potentially underrepresenting certain cooking techniques or regional variations
2. **Action Detection Accuracy**: While our clustering and comparison stages achieve high accuracy (79.1% label reduction, 13.6% segment consolidation), some subtle or ambiguous actions may still be misclassified
3. **Modality Constraints**: Stage 1 operates on **visual information only**—audio cues, verbal instructions, and background conversations are not utilized, potentially missing important contextual information
4. **Regional Scope**: Currently scoped to 12 Indian biryani variations; generalization to other regional cuisines requires retraining and validation
5. **Dataset Size**: While 120 high-quality videos provide good coverage, scaling to hundreds or thousands of videos may reveal edge cases and failure modes

However, these limitations offer **clear directions for enhancement**:

- Systematic biases can be addressed through targeted training on diverse culinary datasets
- Multimodal approaches integrating audio and text could improve detection of subtle variations
- Cross-cuisine transfer learning could extend applicability beyond biryani

---

## Future Work

Building on this foundation, several promising directions emerge:

1. **Cuisine Expansion**: Extend the framework to other Indian cuisines (dosa, sambar, rasam) and international dishes, creating a comprehensive culinary video understanding system
2. **Fine-Grained Temporality**: Improve detection of subtle variations in cooking time, temperature transitions, and ingredient sequencing that distinguish master chefs from novices
3. **Cross-Recipe Reasoning**: Develop deeper multi-video reasoning that can explain **why** certain techniques appear in specific regional variations (e.g., cooking sequence differences between Hyderabadi and Lucknowi biryani)
4. **Interactive Applications**: Build interactive learning platforms where users can query videos, compare techniques, and follow step-by-step cooking guides aligned to their skill level
5. **Chef-Specific Profiling**: Identify and track individual chef styles, enabling the system to recommend videos matching a learner's preferred cooking approach
6. **Nutritional and Ingredient Analysis**: Integrate ingredient-level analysis to provide nutritional information and substitution recommendations
7. **Multilingual QA**: Expand QA generation and answering to Indian languages (Hindi, Tamil, Telugu, Kannada), making culinary education accessible to broader audiences

---

## Citation

If you use the biryani dataset or methodology in your research, please cite our paper:

```bibtex
@inproceedings{rishi2025biryani,
  title={How Does India Cook Biryani? A Video Understanding Dataset for Regional Cuisine Analysis},
  author={Rishi, C.V. and S, Farzana and Goel, Shubham and Arun, Aditya and Jawahar, C.V.},
  booktitle={Proceedings of the 16th Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP'25)},
  month={December},
  year={2025},
  address={Mandi, India}
}
```

---

## Acknowledgements

We thank all the content creators whose biryani cooking videos form the foundation of this dataset. We acknowledge the computational resources provided by IIIT Hyderabad, and thank our annotators for their meticulous work in verifying and curating the video segmentations and QA pairs.
